# Gemma-Fine-tuning-with-LoRA-
This Repository cotains a jupyter notebook that fine-tunes the Gemma-instruct-2b model of goole with Lower Rank Adaptation method.
The main advantage of using LoRA in model training is efficient Memory, CPU, GPU usage when working with LLMs like Gemma.
